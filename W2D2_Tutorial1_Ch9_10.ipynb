{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neuromatch Academy: Week 2, Day 2, Tutorial 1\n",
        "# Modern Convnets\n",
        "\n",
        "__Content creators:__ Laura Pede, Richard Vogg, Marissa Weis, Timo Lüddecke, Alexander Ecker (based on an initial version by Ben Heil)\n",
        "\n",
        "__Content reviewers:__ Arush Tagade, Polina Turishcheva, Yu-Fang Yang. \n",
        "\n",
        "__Content editors:__ Roberto Guidotti, Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Anoop Kulkarni, Roberto Guidotti, Cary Murray, Spiros Chavlis.  \n",
        "\n"
      ],
      "metadata": {
        "id": "Wd4_l9aEeLJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Setup"
      ],
      "metadata": {
        "id": "vlrT1G1ReLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install facenet - a model used to do facial recognition\n",
        "!pip -q install facenet-pytorch"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dxYXu3EaeLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import copy\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import tqdm\n",
        "import urllib\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from matplotlib.colors import ListedColormap\n",
        "from IPython import display\n",
        "from PIL import Image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ejSQALNreLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup GPU device\n",
        "seed = 522\n",
        "random.seed(522)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5pocVODneLJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and prepare the data"
      ],
      "metadata": {
        "id": "H_POdVZ5eLJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Data\n",
        "!git clone --quiet https://github.com/ben-heil/cis_522_data.git\n",
        "!tar -xzf cis_522_data/archive.tar.gz\n",
        "!tar -xzf cis_522_data/faces.tar.gz"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQyKo8HleLJt",
        "execution": {
          "iopub.status.busy": "2021-07-07T09:11:34.567Z",
          "iopub.execute_input": "2021-07-07T09:11:34.626Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Section 9: Face Recognition"
      ],
      "metadata": {
        "id": "Gi4WZE6teLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video 9: Face Recognition using CNNs\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"3q4fKmimZm8\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xYqwcjYqeLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One application of large CNNs is **facial recognition**. The problem formulation in facial recognition is a little different from the image classification we've seen so far. In facial recognition we don't want to have a fixed number of individuals that the model can learn. If that were the case then to learn a new person it would be necessary to modify the output portion of the architecture and retrain to account for the new person.\n",
        "\n",
        "Instead, we train a model to learn an **embedding** where images from the same individual are close to each other in an embedded space, and images corresponding to different people are far apart. When the model is trained, it takes as input an image and outputs an embedding vector corresponding to the image. \n",
        "\n",
        "To achieve this, facial recognitions typically use a triplet loss that compares and two images from the same individual (the \"anchor\" and \"positive\" images) and a negative image from a different individual (the \"negative\" image). The loss requires the distance between the anchor and negative points to be greater than a margin $\\alpha$ + the distance between the anchor and positive points."
      ],
      "metadata": {
        "id": "5Nyrz6dheLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View and transform the data\n",
        "\n",
        "A well-trained facial recognition system should be able to map different images of the same individual relatively close together. We will load 15 images of three individuals (maybe you know them - then you can see that your brain is quite well in facial recognition).\n",
        "\n",
        "After viewing the images, we will transform them: MTCNN detects the face and crops the image around the face. Then we stack all the images together in a tensor."
      ],
      "metadata": {
        "id": "fim20_AfeLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Images\n",
        "#@markdown Here are the source images of Bruce Lee, Neil Patrick Harris, and Pam Grier\n",
        "train_transform = transforms.Compose((transforms.Resize((256, 256)),\n",
        "                                     transforms.ToTensor()))\n",
        "\n",
        "face_dataset = ImageFolder('faces', transform=train_transform)\n",
        "\n",
        "image_count = len(face_dataset)\n",
        "\n",
        "face_loader = torch.utils.data.DataLoader(face_dataset,\n",
        "                                          batch_size=45,\n",
        "                                          shuffle=False)\n",
        "\n",
        "dataiter = iter(face_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(make_grid(images, nrow=15).permute(1,2,0))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "j3h9pt4ueLJ7",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Image Preprocessing Function\n",
        "def process_images(image_dir: str, size = 256):\n",
        "    \"\"\"\n",
        "    This function returns two tensors for the given image dir: one usable for inputting into the\n",
        "    facenet model, and one that is [0,1] scaled for visualizing\n",
        "\n",
        "    Parameters:\n",
        "        image_dir: The glob corresponding to images in a directory\n",
        "\n",
        "    Returns:\n",
        "        model_tensor: A image_count x channels x height x width tensor scaled to between -1 and 1,\n",
        "                      with the faces detected and cropped to the center using mtcnn\n",
        "        display_tensor: A transformed version of the model tensor scaled to between 0 and 1\n",
        "    \"\"\"\n",
        "    mtcnn = MTCNN(image_size=size, margin=32)\n",
        "    images = []\n",
        "    for img_path in glob.glob(image_dir):\n",
        "        img = Image.open(img_path)\n",
        "        # Normalize and crop image\n",
        "        img_cropped = mtcnn(img)\n",
        "        images.append(img_cropped)\n",
        "\n",
        "    model_tensor = torch.stack(images)\n",
        "    display_tensor = model_tensor / (model_tensor.max() * 2)\n",
        "    display_tensor += .5\n",
        "\n",
        "    return model_tensor, display_tensor"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AgGu92oReLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our images loaded, we need to preprocess them. To make the images easier for the network to learn, we crop them to include just faces."
      ],
      "metadata": {
        "id": "PmDMgMDPeLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bruce_tensor, bruce_display = process_images('faces/bruce/*.jpg')\n",
        "neil_tensor, neil_display = process_images('faces/neil/*.jpg')\n",
        "pam_tensor, pam_display = process_images('faces/pam/*.jpg')\n",
        "\n",
        "\n",
        "display_tensor = torch.cat((bruce_display, neil_display, pam_display))\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(make_grid(display_tensor, nrow=15).permute(1, 2, 0, ))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "btOt0T1yeLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding with a pretrained network \n",
        "\n",
        "We load a pretrained facial recognition model called [FaceNet](https://github.com/timesler/facenet-pytorch). It was trained on the [VGGFace2](https://github.com/ox-vgg/vgg_face2) dataset which contains 3.31 million images of 9131 individuals.\n",
        "\n",
        "We use the pretrained model to calculate embeddings for all of our input images."
      ],
      "metadata": {
        "id": "LsMEHUFreLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "c9frh8CBeLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate embedding\n",
        "resnet.classify = False\n",
        "bruce_embeddings = resnet(bruce_tensor.to(device))\n",
        "neil_embeddings = resnet(neil_tensor.to(device))\n",
        "pam_embeddings = resnet(pam_tensor.to(device))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "T8ZAiq4ieLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Think!\n",
        "\n",
        "We want to understand what happens the model receives an image and returns the corresponding embedding vector.\n",
        "\n",
        "- What are the height, width and number of channels of one input image?\n",
        "- What are the dimensions of one stack of images (e.g. bruce_tensor)?\n",
        "- What are the dimensions of the corresponding embedding (e.g. bruce_embeddings)?\n",
        "- What would be the dimensions of the embedding of one input image?\n",
        "\n",
        "\n",
        "Hints: \n",
        "- You can double click on a variable name and hover over it to see the dimensions of tensors.\n",
        "- You do not have to answer the questions in the order they are asked."
      ],
      "metadata": {
        "id": "D_8S-6a9eLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_image = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RtjHvHunDwvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_of_images = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hjI0sJa8D3YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stack_embedding = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "duJZK4peD7Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_embedding = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6_EzNJKCEE0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove explanation\n",
        "\n",
        "\"\"\"\n",
        "1. height: 256, width: 256, channels: 3 (RGB)\n",
        "2. 15x3x256x256\n",
        "3. 15x512\n",
        "4. 1x512 or just 512\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "aVrbngMDhuRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot show 512-dimensional vectors visually, but using **Principal Component Analysis (PCA)** we can project the 512 dimensions onto a 2-dimensional space while preserving the maximum amount of data variation possible. This is just a visual aid for us to understand the concept. If you would do any caluclation, like distances between two images, this would be done with the whole 512-dimensional embedding vectors."
      ],
      "metadata": {
        "id": "JtBFAfR7eLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_tensor = torch.cat((bruce_embeddings, neil_embeddings, pam_embeddings)).to(device = 'cpu')\n",
        "pca = sklearn.decomposition.PCA(n_components=2)\n",
        "pca_tensor = pca.fit_transform(embedding_tensor.detach().numpy())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9Z2g-u9keLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['blue'] * 15 + ['orange'] * 15 + ['magenta'] * 15\n",
        "\n",
        "plt.scatter(pca_tensor[:,0], pca_tensor[:,1], c=colors, marker = 'x')\n",
        "green_patch = mpatches.Patch(color='blue', label='Bruce Lee')\n",
        "orange_patch = mpatches.Patch(color='orange', label='Neil Patrick Harris')\n",
        "purple_patch = mpatches.Patch(color='magenta', label='Pam Grier')\n",
        "\n",
        "plt.title('PCA Representation of the Image Embeddings')\n",
        "plt.legend(handles=[green_patch, orange_patch, purple_patch])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FDnnMADYeLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! The images corresponding to each individual are separated from each other in the embedding space!\n",
        "\n",
        "If Neil Patrick Harris wants to unlock his phone with facial recognition, the phone takes the image from the camera, calculates the embedding and checks if it is close to the registered embeddings corresponding to Neil Patrick Harris."
      ],
      "metadata": {
        "id": "i81EgQtNeLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Section 10: Ethics – bias/discrimination due to pre-training datasets\n",
        "Popular facial recognition datasets like VGGFace2 and CASIA-WebFace consist primarily of caucasian faces. \n",
        "As a result, even state of the art facial recognition models [substantially underperform](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.pdf) when attempting to recognize faces of other races.\n",
        "\n",
        "Given the implications that poor model performance can have in fields like security and criminal justice, it's very important to be aware of these limitations if you're going to be building facial recognition systems.\n",
        "\n",
        "In this example we will work with a small subset from the [UTKFace](https://susanqq.github.io/UTKFace/) dataset with 49 pictures of black women and 49 picture of white women. We will use the same pretrained model as in Section 8, see and discuss the consequences of the model being trained on an imbalanced dataset."
      ],
      "metadata": {
        "id": "HcRRgrtpeLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video 9: Ethical aspects\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"9i8fQwd5fak\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4ygatu5feLJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Data\n",
        "!git clone --quiet https://github.com/richardvogg/face_sample.git\n",
        "!unzip -q face_sample/face_sample2.zip"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UJf8mAC3eLJ8",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load, view and transform the data"
      ],
      "metadata": {
        "id": "aHZko_0ZeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "black_female_tensor, black_female_display = process_images('face_sample2/??_1_1_*.jpg', size = 200)\n",
        "white_female_tensor, white_female_display = process_images('face_sample2/??_1_0_*.jpg', size = 200)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dFZTM_KVeLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the dimensions of these tensors and see that for each group we have images of size 200x200 and three channels (RGB) of 49 individuals."
      ],
      "metadata": {
        "id": "tbafQ51ReLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(white_female_tensor.shape)\n",
        "print(black_female_tensor.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fZDl5pj5eLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize some example faces\n",
        "display_tensor = torch.cat((white_female_display[:15], black_female_display[:15]))\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(make_grid(display_tensor, nrow = 15).permute(1, 2,0,))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VtmGpYO7eLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate embeddings\n",
        "\n",
        "We use the same pretrained facial recognition network as in section 8 to calculate embeddings. If you have memory issues running this part, go to Edit > Notebook settings and check if GPU is selected as Hardware accelerator. If this does not help you can restart the notebook, go to Runtime -> Restart runtime."
      ],
      "metadata": {
        "id": "ATs0xFZNeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet.classify = False\n",
        "black_female_embeddings = resnet(black_female_tensor.to(device))\n",
        "white_female_embeddings = resnet(white_female_tensor.to(device))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1kabP8kueLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the embeddings to show that the model was trained on an imbalanced dataset. For this, we are going to calculate a distance matrix of all combinations of images, like in this small example with n = 3 (in our case n = 98).\n",
        "\n",
        "<img height=500 src=https://raw.githubusercontent.com/richardvogg/face_sample/main/04_DistanceMatrix.png>\n",
        "\n",
        "Calculate the distance between each pair of image embeddings in our tensor and visualize all the distances. Remember that two embeddings are vectors and the distance between two vectors is the Euclidean distance."
      ],
      "metadata": {
        "id": "edJmm163eLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to calculate pairwise distances\n",
        "def calculate_pairwise_distances(embedding_tensor: torch.tensor):\n",
        "    \"\"\"\n",
        "    This function calculates the distance between each pair of image embeddings in a tensor\n",
        "\n",
        "    Parameters:\n",
        "        embedding_tensor: A num_images x embedding_dimension tensor\n",
        "\n",
        "    Returns:\n",
        "        distances: A num_images x num_images tensor containing the pairwise distances between each\n",
        "                   image embedding\n",
        "    \"\"\"\n",
        "\n",
        "    distances = torch.cdist(embedding_tensor, embedding_tensor)\n",
        "\n",
        "    return distances"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "moMrIHfDeLJ8",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize the distances\n",
        "\n",
        "embedding_tensor = torch.cat((black_female_embeddings, white_female_embeddings)).to(device = 'cpu')\n",
        "\n",
        "distances = calculate_pairwise_distances(embedding_tensor)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(distances.detach().numpy())\n",
        "plt.annotate('Black female', (2,-0.5), fontsize=24, va='bottom')\n",
        "plt.annotate('White female', (52,-0.5), fontsize=24, va='bottom')\n",
        "plt.annotate('Black female', (-0.5, 45), fontsize=24, rotation=90, ha='right')\n",
        "plt.annotate('White female', (-0.5, 90), fontsize=24, rotation=90, ha='right')\n",
        "plt.colorbar()\n",
        "plt.axis('off')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3p_xAM8yeLJ8",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 10.1\n",
        "\n",
        "What do you observe? The faces of which group are more similar to each other for the Face Detection algorithm?"
      ],
      "metadata": {
        "id": "isrcSn1heLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "07fFVUMaeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove explanation\n",
        "\n",
        "\"\"\"\n",
        "The distances between black female embeddings are generally lower than the distances between white female embeddings, i.e. for the Face Detection algorithm faces of black females are more similar to each other.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "uq3dq8Wj9sCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 9.2\n",
        "- What does it mean in real life applications that the distance is smaller between the embeddings of one group?\n",
        "- Can you come up with example situations/applications where this has a negative impact?\n",
        "- What could you do to avoid these problems?"
      ],
      "metadata": {
        "id": "yibVyo_FeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ethics_discussion = '' #@param {type:\"string\"}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ty4cUhiIeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove explanation\n",
        "\n",
        "\"\"\"\n",
        "1. Algorithms will have problems to distinguish people of this group if the distances of the embeddings are smaller.\n",
        "2. Many examples possible\n",
        "  - Unblocking the smartphone with a face recognition system might not work or not be secure.\n",
        "  - Surveillance cameras might confuse indivduals.\n",
        "  - Social network automated tagging might confuse individuals.\n",
        "3. Train the model on a balanced dataset with enough samples for each minority. If you use pre-trained models, obtain information about the datasets they were pretrained on.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SbAv4DQ-95PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, to show the importance of the dataset which you use to pretrain your model, look at how much space white men and women take in different embeddings. FairFace is a dataset which is specifically created with completely balanced classes. The blue dots in all visualizations are white male and white female."
      ],
      "metadata": {
        "id": "L4JrTz-deLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=https://i.imgur.com/hCdCBOa.png>\n",
        "\n",
        "[Image Source](https://arxiv.org/abs/1908.04913)"
      ],
      "metadata": {
        "id": "htp3iFFceLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus (optional): Within Sum of Squares\n",
        "\n",
        "We can try to put this observation in numbers. For this we work with the embeddings.\n",
        "We want to calculate the centroid of each group, which is the average of the 49 embeddings of the group. As each embedding vector has a dimension of 512, the centroid will also have this dimension.\n",
        "\n",
        "Now we can calculate how far away the observations of each group $S_i$ are from the centroid $\\mu_i$. This concept is known as Within Sum of Squares (WSS) from cluster analysis.\n",
        "\n",
        "$ \\text{WSS} = \\sum_{x\\in S_i} ||x - \\mu_i||^2$\n",
        "\n",
        "where ||.|| is the Euclidean norm.\n",
        "\n",
        "If the WSS is small, all elements of a group are close to each other. If WSS is larger, they are further away from each other.\n"
      ],
      "metadata": {
        "id": "fGno5KwpeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Function to calculate WSS\n",
        "\n",
        "def wss(group):\n",
        "  \"\"\"\n",
        "    This function returns the sum of squared distances of the N vectors of a\n",
        "    group tensor (N x K) to its centroid (1 x K).\n",
        "\n",
        "    Parameters:\n",
        "        group: A image_count x embedding_size tensor\n",
        "\n",
        "    Returns:\n",
        "        sum_sq: A 1x1 tensor with the sum of squared distances.\n",
        "\n",
        "    Hints:\n",
        "        - to calculate the centroid, torch.mean() will be of use.\n",
        "        - We need the mean of the N=49 observations. If our input tensor is of size\n",
        "          N x K, we expect the centroid to be of dimensions 1 x K.\n",
        "          Use the axis argument within torch.mean\n",
        "    \"\"\"\n",
        "  centroid = torch.mean(group, axis = 0)\n",
        "  distance = torch.linalg.norm(group - centroid.view(1,-1), axis = 1)\n",
        "  sum_sq = torch.sum(distance**2)\n",
        "  return sum_sq"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VIbFD7aceLJ8",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Let's calculate the WSS for the two groups of our example.\n",
        "\n",
        "print(\"Black female embedding WSS: \" + str(round(wss(black_female_embeddings).item(), 2)))\n",
        "print(\"White female embedding WSS: \" + str(round(wss(white_female_embeddings).item(), 2)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QkMdJTUnAn5G",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "bT4bTgFTeLJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video 10: Summary and Outlook\n",
        "from IPython.display import YouTubeVideo\n",
        "video = YouTubeVideo(id=\"MdD6DzqLrLY\", width=854, height=480, fs=1)\n",
        "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "\n",
        "video"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cvWaJ-PVeLJ8"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "W2D2_Tutorial1_Ch8_9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}